{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#Installing dependencies\n!pip install music21\n!apt-get install -y lilypond","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-11T11:16:18.676798Z","iopub.execute_input":"2021-10-11T11:16:18.677034Z","iopub.status.idle":"2021-10-11T11:17:00.625141Z","shell.execute_reply.started":"2021-10-11T11:16:18.676965Z","shell.execute_reply":"2021-10-11T11:17:00.624313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">LSTM'S ALBUM RELEASE</p>\n\n<img src=\"https://github.com/KarnikaKapoor/Files/blob/main/music1.gif?raw=true\">\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB;\">In one of my previous notebooks, I created an RNN that generates lyrics. For such projects, we feed the network a series of strings and the network predicts the next string in the series based on the information it is trained on. This time I decided to use the same principle on the music. \nFull disclosure I am not a musician. However, I know a tiny bit of music theory and play the ukulele for fun. Nonetheless, If you are a musical novice, don't shy away dive right into this notebook. </p> \n\n<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n <p style=\"font-family:newtimeroman;color:#97BACB#97BACB;font-size:120%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS</p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. DATA EXPLORATION](#3)  \n    \n* [4. DATA PREPREPROCESSING](#4)  \n    \n* [5. MODEL BUILDING](#5) \n      \n* [6. EVALUATING MODELS](#6)\n    \n* [7. CONCLUSION](#7)\n    \n* [8. END](#8)\n\n\n<a id=\"1\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">IMPORTING LIBRARIES</p>\n","metadata":{}},{"cell_type":"code","source":"#Importing Libraries\nimport tensorflow \nimport numpy as np \nimport pandas as pd \nfrom collections import Counter\nimport random\nimport IPython\nfrom IPython.display import Image, Audio\nimport music21\nfrom music21 import *\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adamax\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\nnp.random.seed(42)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-11T11:17:00.627086Z","iopub.execute_input":"2021-10-11T11:17:00.627369Z","iopub.status.idle":"2021-10-11T11:17:05.797137Z","shell.execute_reply.started":"2021-10-11T11:17:00.627333Z","shell.execute_reply":"2021-10-11T11:17:05.796307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">LOADING DATA</p>\n\nFor this project, I will be using MIDI files of classical piano music. The dataset includes various artists. I will be working with Frédéric Chopin's compositions. \n \n* First of all, I make a list of all the songs in the Chopin folder parsed as music21 stream.\n\n* Then I will be creating a function to extract chords and notes out of the data creating a corpus.\n\n**Laoding and parsing data**","metadata":{}},{"cell_type":"code","source":"#Loading the list of chopin's midi files as stream \nfilepath = \"../input/classical-music-midi/chopin/\"\n#Getting midi files\nall_midis= []\nfor i in os.listdir(filepath):\n    if i.endswith(\".mid\"):\n        tr = filepath+i\n        midi = converter.parse(tr)\n        all_midis.append(midi)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:17:05.798486Z","iopub.execute_input":"2021-10-11T11:17:05.798866Z","iopub.status.idle":"2021-10-11T11:22:02.230662Z","shell.execute_reply.started":"2021-10-11T11:17:05.79883Z","shell.execute_reply":"2021-10-11T11:22:02.229854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I get the components out of these streams of MIDI files. The midi files only have the piano included as mentioned in the dataset. So the components of the file would be either piano chords or piano notes. \n\n**Note:** The musical notes are the building blocks of the music. It pertains to a pitch associated with a specific audio vibration. Western music utilizes twelve musical notes. \n\n**Chord:** A group of notes that sound good together is a chord.\n\nThe music21 stream that was created in the above cell contains both, chords and notes, we will extract them in the form of notes and obtain a series of notes in the musical composition.\n\n**The function to get the notes:**","metadata":{}},{"cell_type":"code","source":"#Helping function        \ndef extract_notes(file):\n    notes = []\n    pick = None\n    for j in file:\n        songs = instrument.partitionByInstrument(j)\n        for part in songs.parts:\n            pick = part.recurse()\n            for element in pick:\n                if isinstance(element, note.Note):\n                    notes.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n\n    return notes\n#Getting the list of notes as Corpus\nCorpus= extract_notes(all_midis)\nprint(\"Total notes in all the Chopin midis in the dataset:\", len(Corpus))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:02.232844Z","iopub.execute_input":"2021-10-11T11:22:02.233115Z","iopub.status.idle":"2021-10-11T11:22:21.07334Z","shell.execute_reply.started":"2021-10-11T11:22:02.233082Z","shell.execute_reply":"2021-10-11T11:22:21.072583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have our data in the form of a corpus. A list of strings, if you will. Each string indicates a musical note. Let us explore this data corpus.\n\n<a id=\"3\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA EXPLORATION</p>\n\n**In this section, I will be:**\n* Exploring the data Corpus\n* Examine all the notes in the Corpus \n* Simplifying our Corpus to Built a working model\n\n**Let us have a look at the first 50 values in our corpus**","metadata":{}},{"cell_type":"code","source":"print(\"First fifty values in the Corpus:\", Corpus[:50])","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:21.074693Z","iopub.execute_input":"2021-10-11T11:22:21.075113Z","iopub.status.idle":"2021-10-11T11:22:21.080474Z","shell.execute_reply.started":"2021-10-11T11:22:21.075078Z","shell.execute_reply":"2021-10-11T11:22:21.079728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All these values indicate the notes, as mentioned above.\n\n**Printing the music sheet**","metadata":{}},{"cell_type":"code","source":"#First Lets write some functions that we need to look into the data\ndef show(music):\n    display(Image(str(music.write(\"lily.png\"))))\n    \ndef chords_n_notes(Snippet):\n    Melody = []\n    offset = 0 #Incremental\n    for i in Snippet:\n        #If it is chord\n        if (\".\" in i or i.isdigit()):\n            chord_notes = i.split(\".\") #Seperating the notes in chord\n            notes = [] \n            for j in chord_notes:\n                inst_note=int(j)\n                note_snip = note.Note(inst_note)            \n                notes.append(note_snip)\n                chord_snip = chord.Chord(notes)\n                chord_snip.offset = offset\n                Melody.append(chord_snip)\n        # pattern is a note\n        else: \n            note_snip = note.Note(i)\n            note_snip.offset = offset\n            Melody.append(note_snip)\n        # increase offset each iteration so that notes do not stack\n        offset += 1\n    Melody_midi = stream.Stream(Melody)   \n    return Melody_midi\n\nMelody_Snippet = chords_n_notes(Corpus[:100])\nshow(Melody_Snippet)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:21.081966Z","iopub.execute_input":"2021-10-11T11:22:21.082389Z","iopub.status.idle":"2021-10-11T11:22:22.600018Z","shell.execute_reply.started":"2021-10-11T11:22:21.082356Z","shell.execute_reply":"2021-10-11T11:22:22.599285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Playing the above sheet music** \n\n*As I could not play a midi file on the Kaggle interface, I have created a \".wav\" filetype of the same outside of this code. I am using it to create an audio interface. Let us have a listen to the data corpus.* ","metadata":{}},{"cell_type":"code","source":"#to play audio or corpus\nprint(\"Sample Audio From Data\")\nIPython.display.Audio(\"../input/music-generated-lstm/Corpus_Snippet.wav\") ","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:22.601858Z","iopub.execute_input":"2021-10-11T11:22:22.602386Z","iopub.status.idle":"2021-10-11T11:22:23.197889Z","shell.execute_reply.started":"2021-10-11T11:22:22.602347Z","shell.execute_reply":"2021-10-11T11:22:23.197043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Examine all the notes in the Corpus** ","metadata":{}},{"cell_type":"code","source":"#Creating a count dictionary\ncount_num = Counter(Corpus)\nprint(\"Total unique notes in the Corpus:\", len(count_num))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:23.199046Z","iopub.execute_input":"2021-10-11T11:22:23.199377Z","iopub.status.idle":"2021-10-11T11:22:23.225688Z","shell.execute_reply.started":"2021-10-11T11:22:23.199335Z","shell.execute_reply":"2021-10-11T11:22:23.224781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the notes dictionary\nNotes = list(count_num.keys())\nRecurrence = list(count_num.values())\n#Average recurrenc for a note in Corpus\ndef Average(lst):\n    return sum(lst) / len(lst)\nprint(\"Average recurrenc for a note in Corpus:\", Average(Recurrence))\nprint(\"Most frequent note in Corpus appeared:\", max(Recurrence), \"times\")\nprint(\"Least frequent note in Corpus appeared:\", min(Recurrence), \"time\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:23.227296Z","iopub.execute_input":"2021-10-11T11:22:23.227778Z","iopub.status.idle":"2021-10-11T11:22:23.235741Z","shell.execute_reply.started":"2021-10-11T11:22:23.227728Z","shell.execute_reply":"2021-10-11T11:22:23.235081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, there are some very rare notes in the melody; some so rare that it was played only once in the whole data. This would create a lot of problems. (I did run into most of them while writing this piece)\nTo spare us the error reports, let us have a look at the frequency of the notes. \nAnd for simplicity, I shall be eliminating some of the least occurring notes. I am sure Chopin wouldn't mind me messing with his masterpiece for science or would he? Either way, I may never know!   ","metadata":{}},{"cell_type":"code","source":"# Plotting the distribution of Notes\nplt.figure(figsize=(18,3),facecolor=\"#97BACB\")\nbins = np.arange(0,(max(Recurrence)), 50) \nplt.hist(Recurrence, bins=bins, color=\"#97BACB\")\nplt.axvline(x=100,color=\"#DBACC1\")\nplt.title(\"Frequency Distribution Of Notes In The Corpus\")\nplt.xlabel(\"Frequency Of Chords in Corpus\")\nplt.ylabel(\"Number Of Chords\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:23.2393Z","iopub.execute_input":"2021-10-11T11:22:23.239962Z","iopub.status.idle":"2021-10-11T11:22:23.648745Z","shell.execute_reply.started":"2021-10-11T11:22:23.239927Z","shell.execute_reply":"2021-10-11T11:22:23.647902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have decided, I will be taking out the notes that were played less than 100 times. I mean, if Chopin liked them he would have played it a lot more often. So I create a list of rare notes in the next section. ","metadata":{}},{"cell_type":"code","source":"#Getting a list of rare chords\nrare_note = []\nfor index, (key, value) in enumerate(count_num.items()):\n    if value < 100:\n        m =  key\n        rare_note.append(m)\n        \nprint(\"Total number of notes that occur less than 100 times:\", len(rare_note))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:23.650348Z","iopub.execute_input":"2021-10-11T11:22:23.650629Z","iopub.status.idle":"2021-10-11T11:22:23.656827Z","shell.execute_reply.started":"2021-10-11T11:22:23.650595Z","shell.execute_reply":"2021-10-11T11:22:23.655939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eleminating the rare notes\nfor element in Corpus:\n    if element in rare_note:\n        Corpus.remove(element)\n\nprint(\"Length of Corpus after elemination the rare notes:\", len(Corpus))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:23.658297Z","iopub.execute_input":"2021-10-11T11:22:23.658586Z","iopub.status.idle":"2021-10-11T11:22:25.737944Z","shell.execute_reply.started":"2021-10-11T11:22:23.658551Z","shell.execute_reply":"2021-10-11T11:22:25.737173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally! This is the cleaned data Corpus that I will be using for the music generation.  \nIn the next section, I will be preprocessing this Corpus for the training model. \n\nThe workflow for this project involves,\n\n<p style=\"background-color:#EBDDD0;font-family:newtimeroman;color:#444160;text-align:center;font-size:120%;\">Loading Data ➡️ Preprocessing ➡️ Building Mapping Dictionary ➡️ Building Model ➡️ Generating Music</p>\n\nAs I have loaded and explored the data,  I will proceed further by pre-processing the text.  \n\n\n<a id=\"4\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA PREPROCESSING</p>\n\nNotes are basically sound waves. In music, we have certain specific combinations of Frequency and Wavelength standardized as said notes. Our Corpus has the name of that note. As we parsed the data at the time of loading we took the help of the music21 library (by nice people at MIT); The library fetches Frequency, Wavelength, duration etc for the given notes. \n\n\n\n**In this section, I will be performing the following:**\n\n**Creating a dictionary:** Creating a dictionary to map the notes and their indices. We have the note's name as a string the Corpus. For the computer, these names are just a symbol. So we create a dictionary to map each unique note in our Corpus to a number. And vice versa to retrieve the values at the time of prediction. This will be used to encode and decode the information going in and getting out of the RNN. \n\n**Encoding and Splitting the corpus:** Encoding and splitting the corpus into smaller sequences of equal length: At this point, the Corpus contain notes. We will encode this corpus and create small sequences of equal lengths of features and the corresponding targets. Each feature and target will contain the mapped index in the dictionary of the unique characters they signify. \n\n**Assigning X and y:** The labels are then resized and normalized. Whereas the targets are one-hot encoded. Ready to be sent to the RNN for the training, but before that let us built the RNN model. \n\n**Splitting Train and Seed datasets** To create music we will need to send some input to the RNN. For that, we will set aside a part of the data as seeds. We could have trained it all but I am no musician to come up with an input seed value. \n\n**Creating a list of sorted unique characters**","metadata":{}},{"cell_type":"code","source":"# Storing all the unique characters present in my corpus to bult a mapping dic. \nsymb = sorted(list(set(Corpus)))\n\nL_corpus = len(Corpus) #length of corpus\nL_symb = len(symb) #length of total unique characters\n\n#Building dictionary to access the vocabulary from indices and vice versa\nmapping = dict((c, i) for i, c in enumerate(symb))\nreverse_mapping = dict((i, c) for i, c in enumerate(symb))\n\nprint(\"Total number of characters:\", L_corpus)\nprint(\"Number of unique characters:\", L_symb)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:25.739349Z","iopub.execute_input":"2021-10-11T11:22:25.739783Z","iopub.status.idle":"2021-10-11T11:22:25.749121Z","shell.execute_reply.started":"2021-10-11T11:22:25.739747Z","shell.execute_reply":"2021-10-11T11:22:25.74817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding and Splitting the Corpus as Labels and Targets**","metadata":{}},{"cell_type":"code","source":"#Splitting the Corpus in equal length of strings and output target\nlength = 40\nfeatures = []\ntargets = []\nfor i in range(0, L_corpus - length, 1):\n    feature = Corpus[i:i + length]\n    target = Corpus[i + length]\n    features.append([mapping[j] for j in feature])\n    targets.append(mapping[target])\n    \n    \nL_datapoints = len(targets)\nprint(\"Total number of sequences in the Corpus:\", L_datapoints)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:25.750838Z","iopub.execute_input":"2021-10-11T11:22:25.751166Z","iopub.status.idle":"2021-10-11T11:22:26.011177Z","shell.execute_reply.started":"2021-10-11T11:22:25.751083Z","shell.execute_reply":"2021-10-11T11:22:26.010136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape X and normalize\nX = (np.reshape(features, (L_datapoints, length, 1)))/ float(L_symb)\n# one hot encode the output variable\ny = tensorflow.keras.utils.to_categorical(targets) ","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:26.012606Z","iopub.execute_input":"2021-10-11T11:22:26.01296Z","iopub.status.idle":"2021-10-11T11:22:26.491054Z","shell.execute_reply.started":"2021-10-11T11:22:26.012924Z","shell.execute_reply":"2021-10-11T11:22:26.490304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting Train and Seed datasets**","metadata":{}},{"cell_type":"code","source":"#Taking out a subset of data to be used as seed\nX_train, X_seed, y_train, y_seed = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:26.492546Z","iopub.execute_input":"2021-10-11T11:22:26.492824Z","iopub.status.idle":"2021-10-11T11:22:26.534046Z","shell.execute_reply.started":"2021-10-11T11:22:26.492789Z","shell.execute_reply":"2021-10-11T11:22:26.533177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">MODEL BUILDING</p>\n\nI will be employing an LSTM for this project.\n\n**Following steps are involved in the model building**\n\n* Initialising the Model\n* Defining by adding layers\n* Compiling the Model\n* Training the Model\n\n**Building the Model**","metadata":{}},{"cell_type":"code","source":"#Initialising the Model\nmodel = Sequential()\n#Adding layers\nmodel.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.1))\nmodel.add(LSTM(256))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(y.shape[1], activation='softmax'))\n#Compiling the model for training  \nopt = Adamax(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-11T11:22:26.535608Z","iopub.execute_input":"2021-10-11T11:22:26.536044Z","iopub.status.idle":"2021-10-11T11:22:29.074068Z","shell.execute_reply.started":"2021-10-11T11:22:26.535998Z","shell.execute_reply":"2021-10-11T11:22:29.073278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model's Summary               \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:22:29.075442Z","iopub.execute_input":"2021-10-11T11:22:29.075722Z","iopub.status.idle":"2021-10-11T11:22:29.08651Z","shell.execute_reply.started":"2021-10-11T11:22:29.07569Z","shell.execute_reply":"2021-10-11T11:22:29.085555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training the Model\nhistory = model.fit(X_train, y_train, batch_size=256, epochs=200)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-11T11:22:29.088068Z","iopub.execute_input":"2021-10-11T11:22:29.088315Z","iopub.status.idle":"2021-10-11T11:39:04.559223Z","shell.execute_reply.started":"2021-10-11T11:22:29.088288Z","shell.execute_reply":"2021-10-11T11:39:04.558518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">EVALUATING MODELS</p>\n\nNow that I have my model trained on the MIDI files of piano music, let us see how it performs. \n\n**To evaluate my model, I shall be having a look at:**\n* The performance of the model via Learning Curves\n* The melody created\n\n**Plotting the learning curve for the loss function**","metadata":{}},{"cell_type":"code","source":"#Plotting the learnings \nhistory_df = pd.DataFrame(history.history)\nfig = plt.figure(figsize=(15,4), facecolor=\"#97BACB\")\nfig.suptitle(\"Learning Plot of Model for Loss\")\npl=sns.lineplot(data=history_df[\"loss\"],color=\"#444160\")\npl.set(ylabel =\"Training Loss\")\npl.set(xlabel =\"Epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:39:04.560441Z","iopub.execute_input":"2021-10-11T11:39:04.560925Z","iopub.status.idle":"2021-10-11T11:39:04.933991Z","shell.execute_reply.started":"2021-10-11T11:39:04.560889Z","shell.execute_reply":"2021-10-11T11:39:04.933094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating the Melody**\n\nA function to obtain the generated music","metadata":{}},{"cell_type":"code","source":"def Malody_Generator(Note_Count):\n    seed = X_seed[np.random.randint(0,len(X_seed)-1)]\n    Music = \"\"\n    Notes_Generated=[]\n    for i in range(Note_Count):\n        seed = seed.reshape(1,length,1)\n        prediction = model.predict(seed, verbose=0)[0]\n        prediction = np.log(prediction) / 1.0 #diversity\n        exp_preds = np.exp(prediction)\n        prediction = exp_preds / np.sum(exp_preds)\n        index = np.argmax(prediction)\n        index_N = index/ float(L_symb)   \n        Notes_Generated.append(index)\n        Music = [reverse_mapping[char] for char in Notes_Generated]\n        seed = np.insert(seed[0],len(seed[0]),index_N)\n        seed = seed[1:]\n    #Now, we have music in form or a list of chords and notes and we want to be a midi file.\n    Melody = chords_n_notes(Music)\n    Melody_midi = stream.Stream(Melody)   \n    return Music,Melody_midi\n\n\n#getting the Notes and Melody created by the model\nMusic_notes, Melody = Malody_Generator(100)\nshow(Melody)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:39:04.935517Z","iopub.execute_input":"2021-10-11T11:39:04.93586Z","iopub.status.idle":"2021-10-11T11:39:09.963958Z","shell.execute_reply.started":"2021-10-11T11:39:04.935822Z","shell.execute_reply":"2021-10-11T11:39:09.962264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This sure looks like music! To check if it sounds like music we have to listen to the MIDI file. Playing midi is crumblesome. I have saved and converted a few generated melodies to \".wav\" format outside of this notebook. So let us have a listen. \n\n**Melody Generated Sample 1**","metadata":{}},{"cell_type":"code","source":"#To save the generated melody\nMelody.write('midi','Melody_Generated.mid')\n#to play audio or corpus\nIPython.display.Audio(\"../input/music-generated-lstm/Melody_Generated 2.wav\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:39:09.965793Z","iopub.execute_input":"2021-10-11T11:39:09.966108Z","iopub.status.idle":"2021-10-11T11:39:10.658233Z","shell.execute_reply.started":"2021-10-11T11:39:09.966065Z","shell.execute_reply":"2021-10-11T11:39:10.657254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Melody Generated Sample 2**","metadata":{}},{"cell_type":"code","source":"#to play audio or corpus\nIPython.display.Audio(\"../input/music-generated-lstm/Melody_Generated_1.wav\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T11:39:10.659441Z","iopub.execute_input":"2021-10-11T11:39:10.659716Z","iopub.status.idle":"2021-10-11T11:39:11.306194Z","shell.execute_reply.started":"2021-10-11T11:39:10.659678Z","shell.execute_reply":"2021-10-11T11:39:11.305415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">CONCLUSION</p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB\">On inspecting the generated melody, I am quite satisfied. Unlike the lyrics project, The music doesn't have to hold true to the grammatical syntax. On the question, is it a good musical composition; is it artsy? Did our LSTM create a masterpiece? I don't know! I am not a connoisseur of music. I used a basic RNN and it worked alright.</p> \n\n<img src=\"https://github.com/KarnikaKapoor/Files/blob/main/music2.gif?raw=true\">     \n     \n     \n<p style=\"font-family:newtimeroman;font-size:120%;color:#97BACB\">So I decided to let the model have the fame it deserved. I am releasing the album! It is out on my blog. Don't forget to get your copy!</p>\n\n**Album** [Down The Uncanny Valley: Album Release](https://karnikakapoor.blogspot.com/2021/10/down-uncanny-valley.html)\n\n**Some Useful Resources:**\n\n[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n\n[MuseNet](https://openai.com/blog/musenet/) \n\n[Lyrics Generator](https://www.kaggle.com/karnikakapoor/lyrics-generator-rnn)\n\n\n**<span style=\"color:#DBACC1;\"> If you liked this Notebook, please do upvote.</span>**\n\n**<span style=\"color:#DBACC1;\"> If you have any questions, feel free to comment!</span>**\n\n**<span style=\"color:#DBACC1;\"> Best Wishes!</span>**\n\n<a id=\"8\"></a>\n# <p style=\"background-color:#97BACB;font-family:newtimeroman;color:#EBDDD0;font-size:120%;text-align:center;border-radius:40px 40px;\">END</p>","metadata":{}}]}